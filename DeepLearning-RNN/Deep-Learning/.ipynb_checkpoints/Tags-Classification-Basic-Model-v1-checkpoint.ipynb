{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "import pandas as pd\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(np.random.seed(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[4.17022005e-01 7.20324493e-01 1.14374817e-04 3.02332573e-01]\n",
      "None\n",
      "[4.17022005e-01 7.20324493e-01 1.14374817e-04 3.02332573e-01]\n",
      "[0.14675589 0.09233859 0.18626021 0.34556073]\n",
      "[0.39676747 0.53881673 0.41919451 0.6852195 ]\n"
     ]
    }
   ],
   "source": [
    "print(np.random.seed(1))\n",
    "print(np.random.rand(4))\n",
    "print(np.random.seed(1))\n",
    "print(np.random.rand(4))\n",
    "print(np.random.rand(4))\n",
    "print(np.random.rand(4))\n",
    "#The seed is for when we want repeatable results. If you don't want that, don't seed your generator. \n",
    "#It will use the system time for an elegant random seed. It allows every time to generate the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv('../../train_emoji2.csv', delimiter=',')\n",
    "# df=pd.read_csv('../../train_emoji2.csv', delimiter=',', header=None)\n",
    "# print(df)\n",
    "# print(df[0])\n",
    "# print(df[1])\n",
    "# X_train, Y_train = pd.read_csv('../../train_emoji2.csv', delimiter=',')\n",
    "# print(X_train)\n",
    "# print(Y_train)\n",
    "# X_test, Y_test = pd.read_csv('../../tesss.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd = pd.read_csv('../../dataset2.csv', delimiter=';', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bd\n",
    "X_train = bd[0]\n",
    "Y_train = bd[1]\n",
    "# X_train\n",
    "# Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "planets solar system orbit plane yes excluding pluto way excluded planet observing planets orbit sun seem relatively planar roughly orbit along plane due way solar system formed physical phenomena observed systems  :  orbit\n"
     ]
    }
   ],
   "source": [
    "indice = 73\n",
    "print(X_train[indice], \" : \", Y_train[indice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
    "# convert y to one hot vector; size(1; 10.000=vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('../../glove.6B.50d.txt')\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the index of sun in the vocabulary is 347345\n",
      "the 289846th word in the vocabulary is potatos\n",
      "cucumber GloVe  [-3.7233e-01  1.6680e+00  6.5616e-01  1.2843e+00 -7.0946e-02 -1.6153e+00\n",
      " -2.8654e-01 -6.7317e-01 -4.9887e-01  1.0531e-01  1.8092e-01  4.0495e-01\n",
      "  5.2388e-01  2.8540e-01 -2.5076e-01  7.5149e-01 -1.2153e+00  5.3438e-02\n",
      " -1.0649e+00  3.7125e-02 -2.2833e-01  3.0398e-01  1.2688e+00 -8.5494e-01\n",
      "  4.9330e-01 -9.2463e-01 -1.4650e-01  5.3333e-01  2.0661e-01 -5.4570e-02\n",
      "  2.6193e+00 -1.1424e-01 -3.0039e-01 -1.8384e-03 -7.7640e-01 -5.8229e-01\n",
      " -4.1817e-01 -4.0371e-02  4.1883e-01 -2.3931e-01  2.1250e-01 -1.7315e-01\n",
      " -3.1351e-01  3.6040e-01  7.9981e-01 -1.7144e-01  1.9295e-01 -1.6196e+00\n",
      "  3.4135e-01 -1.7726e-01]\n"
     ]
    }
   ],
   "source": [
    "word = \"sun\"\n",
    "index = 289846\n",
    "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
    "print(\"the\", str(index) + \"th word in the vocabulary is\", index_to_word[index])\n",
    "print(\"cucumber GloVe \" , word_to_vec_map[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.7233e-01  1.6680e+00  6.5616e-01  1.2843e+00 -7.0946e-02 -1.6153e+00\n",
      " -2.8654e-01 -6.7317e-01 -4.9887e-01  1.0531e-01  1.8092e-01  4.0495e-01\n",
      "  5.2388e-01  2.8540e-01 -2.5076e-01  7.5149e-01 -1.2153e+00  5.3438e-02\n",
      " -1.0649e+00  3.7125e-02 -2.2833e-01  3.0398e-01  1.2688e+00 -8.5494e-01\n",
      "  4.9330e-01 -9.2463e-01 -1.4650e-01  5.3333e-01  2.0661e-01 -5.4570e-02\n",
      "  2.6193e+00 -1.1424e-01 -3.0039e-01 -1.8384e-03 -7.7640e-01 -5.8229e-01\n",
      " -4.1817e-01 -4.0371e-02  4.1883e-01 -2.3931e-01  2.1250e-01 -1.7315e-01\n",
      " -3.1351e-01  3.6040e-01  7.9981e-01 -1.7144e-01  1.9295e-01 -1.6196e+00\n",
      "  3.4135e-01 -1.7726e-01]\n"
     ]
    }
   ],
   "source": [
    "print(word_to_vec_map[\"sun\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "    \n",
    "    words = None\n",
    "    words = sentence.lower().split(' ')\n",
    "\n",
    "    avg = None\n",
    "    avg=np.zeros(50)\n",
    "\n",
    "    for w in words:\n",
    "        avg += word_to_vec_map[w]\n",
    "    avg = avg/len(words)\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg =  [-0.008005    0.56370833 -0.50427333  0.258865    0.55131103  0.03104983\n",
      " -0.21013718  0.16893933 -0.09590267  0.141784   -0.15708967  0.18525867\n",
      "  0.6495785   0.38371117  0.21102167  0.11301667  0.02613967  0.26037767\n",
      "  0.05820667 -0.01578167 -0.12078833 -0.02471267  0.4128455   0.5152061\n",
      "  0.38756167 -0.898661   -0.535145    0.33501167  0.68806933 -0.2156265\n",
      "  1.797155    0.10476933 -0.36775333  0.750785    0.10282583  0.348925\n",
      " -0.27262833  0.66768    -0.10706167 -0.283635    0.59580117  0.28747333\n",
      " -0.3366635   0.23393817  0.34349183  0.178405    0.1166155  -0.076433\n",
      "  0.1445417   0.09808667]\n"
     ]
    }
   ],
   "source": [
    "avg = sentence_to_avg(\"Morrocan couscous is my favorite dish\", word_to_vec_map)\n",
    "print(\"avg = \", avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def sentence_to_avg(sentence, word_to_vec_map):\n",
      "    \n",
      "    words = None\n",
      "    words = sentence.lower().split(' ')\n",
      "\n",
      "    avg = None\n",
      "    avg=np.zeros(50)\n",
      "\n",
      "    for w in words:\n",
      "        avg += word_to_vec_map[w]\n",
      "    avg = avg/len(words)\n",
      "    \n",
      "    return avg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "lines = inspect.getsource(sentence_to_avg)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.19474625  0.8112481   0.0991297   0.5103457   0.2786908  -0.2147849\n",
      " -0.254642   -0.2051564   0.171427    0.1218676   0.2312191   0.2719882\n",
      "  0.138674   -0.0992469   0.401583    0.4369414   0.243575    0.0860508\n",
      " -0.7386922  -0.6418728  -0.0311067   0.1205435   0.6660646  -0.553697\n",
      "  0.431729   -1.230056   -0.5740626   0.4202863   0.437054   -0.0913605\n",
      "  2.64779     0.0246364   0.0083785  -0.12630128 -0.3416827  -0.305015\n",
      " -0.433997    0.2143688   0.156636   -0.398319   -0.0441471  -0.1506397\n",
      "  0.4003463   0.167181    0.2169563  -0.119635   -0.1377633  -0.55953082\n",
      "  0.0229531  -0.415814  ]\n"
     ]
    }
   ],
   "source": [
    "print(sentence_to_avg(\"much gold sun claims gold sun water oceans really true\", word_to_vec_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(129,)\n",
      "(36,)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train.shape)\n",
    "print(Y_train.unique().shape)\n",
    "Y_train_unique = np.sort(Y_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAGS \n",
      "\n",
      "0 sun\n",
      "1 star\n",
      "2 sun\n",
      "3 waves\n",
      "4 distances\n",
      "5 moon\n",
      "6 moon\n",
      "7 asteroids\n",
      "8 moon\n",
      "9 sun\n",
      "10 galaxy\n",
      "11 sun\n",
      "12 exoplanet\n",
      "13 solar\n",
      "14 space\n",
      "15 observing\n",
      "16 cosmology\n",
      "17 solar\n",
      "18 atmosphere\n",
      "19 hole\n",
      "20 star\n",
      "21 moon\n",
      "22 star\n",
      "23 solar\n",
      "24 galaxy\n",
      "25 hole\n",
      "26 universe\n",
      "27 sun\n",
      "28 solar\n",
      "29 moon\n",
      "30 gravity\n",
      "31 inclination\n",
      "32 gravity\n",
      "33 planet\n",
      "34 sun\n",
      "35 orbit\n",
      "36 jupiter\n",
      "37 hole\n",
      "38 telescope\n",
      "39 solar\n",
      "40 milky\n",
      "41 sun\n",
      "42 relativity\n",
      "43 solar\n",
      "44 light\n",
      "45 atmosphere\n",
      "46 cosmology\n",
      "47 gravity\n",
      "48 moon\n",
      "49 pluto\n",
      "50 gravity\n",
      "51 moon\n",
      "52 supernova\n",
      "53 planet\n",
      "54 orbit\n",
      "55 hole\n",
      "56 supernova\n",
      "57 cosmology\n",
      "58 hole\n",
      "59 saturn\n",
      "60 cosmology\n",
      "61 star\n",
      "62 light\n",
      "63 hole\n",
      "64 distances\n",
      "65 sun\n",
      "66 hole\n",
      "67 solar\n",
      "68 sun\n",
      "69 gravity\n",
      "70 universe\n",
      "71 sun\n",
      "72 star\n",
      "73 orbit\n",
      "74 galaxy\n",
      "75 theory\n",
      "76 earth\n",
      "77 star\n",
      "78 sun\n",
      "79 observing\n",
      "80 gravity\n",
      "81 distances\n",
      "82 telescope\n",
      "83 moon\n",
      "84 waves\n",
      "85 neutron\n",
      "86 venus\n",
      "87 supernova\n",
      "88 sun\n",
      "89 satellites\n",
      "90 orbit\n",
      "91 observing\n",
      "92 star\n",
      "93 universe\n",
      "94 moon\n",
      "95 distances\n",
      "96 exoplanet\n",
      "97 solar\n",
      "98 light\n",
      "99 star\n",
      "100 galaxy\n",
      "101 gravity\n",
      "102 orbit\n",
      "103 expansion\n",
      "104 universe\n",
      "105 star\n",
      "106 coordinate\n",
      "107 rotation\n",
      "108 telescope\n",
      "109 sun\n",
      "110 observing\n",
      "111 telescope\n",
      "112 star\n",
      "113 planet\n",
      "114 sun\n",
      "115 orbit\n",
      "116 galaxy\n",
      "117 telescope\n",
      "118 hole\n",
      "119 hole\n",
      "120 orbit\n",
      "121 planet\n",
      "122 moon\n",
      "123 eclipse\n",
      "124 cosmology\n",
      "125 star\n",
      "126 cosmology\n",
      "127 earth\n",
      "128 solar\n",
      "\n",
      "UNIQUE TAGS \n",
      "\n",
      "0 asteroids\n",
      "1 atmosphere\n",
      "2 coordinate\n",
      "3 cosmology\n",
      "4 distances\n",
      "5 earth\n",
      "6 eclipse\n",
      "7 exoplanet\n",
      "8 expansion\n",
      "9 galaxy\n",
      "10 gravity\n",
      "11 hole\n",
      "12 inclination\n",
      "13 jupiter\n",
      "14 light\n",
      "15 milky\n",
      "16 moon\n",
      "17 neutron\n",
      "18 observing\n",
      "19 orbit\n",
      "20 planet\n",
      "21 pluto\n",
      "22 relativity\n",
      "23 rotation\n",
      "24 satellites\n",
      "25 saturn\n",
      "26 solar\n",
      "27 space\n",
      "28 star\n",
      "29 sun\n",
      "30 supernova\n",
      "31 telescope\n",
      "32 theory\n",
      "33 universe\n",
      "34 venus\n",
      "35 waves\n"
     ]
    }
   ],
   "source": [
    "print(\"TAGS \\n\")\n",
    "for y,i in enumerate(Y_train):\n",
    "    print(y,i)\n",
    "print(\"\\nUNIQUE TAGS \\n\")\n",
    "for y,i in enumerate(Y_train_unique):\n",
    "    print(y,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<class 'numpy.int64'>\n",
      "[29 28 29 35  4 16 16  0 16 29  9 29  7 26 27 18  3 26  1 11 28 16 28 26\n",
      "  9 11 33 29 26 16 10 12 10 20 29 19 13 11 31 26 15 29 22 26 14  1  3 10\n",
      " 16 21 10 16 30 20 19 11 30  3 11 25  3 28 14 11  4 29 11 26 29 10 33 29\n",
      " 28 19  9 32  5 28 29 18 10  4 31 16 35 17 34 30 29 24 19 18 28 33 16  4\n",
      "  7 26 14 28  9 10 19  8 33 28  2 23 31 29 18 31 28 20 29 19  9 31 11 11\n",
      " 19 20 16  6  3 28  3  5 26]\n"
     ]
    }
   ],
   "source": [
    "Y_train_converted_to_numbers = np.zeros(shape=(Y_train.shape[0],)).astype(int)\n",
    "print(Y_train_converted_to_numbers[0])\n",
    "print(type(Y_train_converted_to_numbers[0]))\n",
    "for index, element in enumerate(Y_train):\n",
    "#     Y_train_unique_converted_to_numbers[i]=\n",
    "    Y_train_converted_to_numbers[index] = int(np.where(Y_train_unique == element)[0])\n",
    "print(Y_train_converted_to_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C, dtype=int)[Y.reshape(-1)]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(129, 36)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "(array([29]),)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "Y_one_hot = convert_to_one_hot(Y_train_converted_to_numbers, Y_train_unique.shape[0])\n",
    "print(Y_one_hot.shape)\n",
    "print(Y_one_hot)\n",
    "print(np.where(Y_one_hot[0] == 1))\n",
    "print(Y_one_hot[0][20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    ps = np.exp(z)\n",
    "    ps /= np.sum(ps)\n",
    "    return ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, Y, W, b, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Given X (sentences) and Y (emoji indices), predict emojis and compute the accuracy of your model over the given set.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data containing sentences, numpy array of shape (m, None)\n",
    "    Y -- labels, containing index of the label emoji, numpy array of shape (m, 1)\n",
    "    \n",
    "    Returns:\n",
    "    pred -- numpy array of shape (m, 1) with your predictions\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    pred = np.zeros((m, 1))\n",
    "    \n",
    "    for j in range(m):                       # Loop over training examples\n",
    "        \n",
    "        # Split jth test example (sentence) into list of lower case words\n",
    "        words = X[j].lower().split()\n",
    "        \n",
    "        # Average words' vectors\n",
    "        avg = np.zeros((50,))\n",
    "        for w in words:\n",
    "            avg += word_to_vec_map[w]\n",
    "        avg = avg/len(words)\n",
    "\n",
    "        # Forward propagation\n",
    "        Z = np.dot(W, avg) + b\n",
    "        A = softmax(Z)\n",
    "        pred[j] = np.argmax(A)\n",
    "        \n",
    "    print(\"Accuracy: \"  + str(np.mean((pred[:] == Y.reshape(Y.shape[0],1)[:]))))\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 800):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "\n",
    "    m = Y.shape[0]                          \n",
    "    n_y = Y_train_unique.shape[0]                                \n",
    "    n_h = 50                                \n",
    "    \n",
    "    # Initialize parameters using Xavier initialization\n",
    "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
    "    b = np.zeros((n_y,))\n",
    "    # Convert Y to Y_onehot with n_y classes\n",
    "    Y_oh = convert_to_one_hot(Y, C = n_y)\n",
    "    # Optimization loop\n",
    "    for t in range(num_iterations):\n",
    "        for i in range(m):\n",
    "            # Loop over the number of iterations\n",
    "            # Loop over the training examples\n",
    "            ### START CODE HERE ### (â‰ˆ 4 lines of code)\n",
    "            # Average the word vectors of the words from the i'th training example\n",
    "            avg = sentence_to_avg(X[i], word_to_vec_map)\n",
    "            # Forward propagate the avg through the softmax layer\n",
    "            z = np.dot(W, avg) + b\n",
    "            a = softmax(z)\n",
    "            # Compute cost using the i'th training label's one hot representation and \"A\" (theoutput of the softmax)\n",
    "            cost = - np.sum(Y_oh[i] * np.log(a))\n",
    "            ### END CODE HERE ###\n",
    "            # Compute gradients\n",
    "            dz = a - Y_oh[i]\n",
    "            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
    "            db = dz\n",
    "            # Update parameters with Stochastic Gradient Descent\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "#         print(t)\n",
    "        if t % 100 == 0:\n",
    "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
    "            pred = predict(X, Y, W, b, word_to_vec_map)\n",
    "    return pred, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = 2.745146780456182\n",
      "Accuracy: 0.13178294573643412\n",
      "Epoch: 100 --- cost = 2.30999785698811\n",
      "Accuracy: 0.5503875968992248\n",
      "Epoch: 200 --- cost = 2.222559654125796\n",
      "Accuracy: 0.6976744186046512\n",
      "Epoch: 300 --- cost = 2.1511878285596953\n",
      "Accuracy: 0.813953488372093\n",
      "Epoch: 400 --- cost = 2.0630006071094207\n",
      "Accuracy: 0.8914728682170543\n",
      "Epoch: 500 --- cost = 1.9574114916850016\n",
      "Accuracy: 0.9224806201550387\n",
      "Epoch: 600 --- cost = 1.8404674663917744\n",
      "Accuracy: 0.9457364341085271\n",
      "Epoch: 700 --- cost = 1.7188073527728425\n",
      "Accuracy: 0.9534883720930233\n",
      "[[29.]\n",
      " [28.]\n",
      " [29.]\n",
      " [35.]\n",
      " [ 4.]\n",
      " [16.]\n",
      " [16.]\n",
      " [ 0.]\n",
      " [16.]\n",
      " [29.]\n",
      " [ 9.]\n",
      " [29.]\n",
      " [ 7.]\n",
      " [26.]\n",
      " [27.]\n",
      " [18.]\n",
      " [ 3.]\n",
      " [26.]\n",
      " [ 1.]\n",
      " [11.]\n",
      " [28.]\n",
      " [16.]\n",
      " [28.]\n",
      " [26.]\n",
      " [ 9.]\n",
      " [11.]\n",
      " [33.]\n",
      " [26.]\n",
      " [26.]\n",
      " [16.]\n",
      " [10.]\n",
      " [12.]\n",
      " [10.]\n",
      " [29.]\n",
      " [29.]\n",
      " [19.]\n",
      " [13.]\n",
      " [11.]\n",
      " [31.]\n",
      " [26.]\n",
      " [15.]\n",
      " [29.]\n",
      " [22.]\n",
      " [26.]\n",
      " [14.]\n",
      " [ 1.]\n",
      " [ 3.]\n",
      " [10.]\n",
      " [16.]\n",
      " [21.]\n",
      " [10.]\n",
      " [16.]\n",
      " [28.]\n",
      " [20.]\n",
      " [19.]\n",
      " [11.]\n",
      " [30.]\n",
      " [ 3.]\n",
      " [11.]\n",
      " [25.]\n",
      " [ 3.]\n",
      " [28.]\n",
      " [14.]\n",
      " [11.]\n",
      " [ 4.]\n",
      " [29.]\n",
      " [11.]\n",
      " [26.]\n",
      " [29.]\n",
      " [10.]\n",
      " [33.]\n",
      " [29.]\n",
      " [28.]\n",
      " [19.]\n",
      " [ 9.]\n",
      " [32.]\n",
      " [ 5.]\n",
      " [28.]\n",
      " [29.]\n",
      " [18.]\n",
      " [10.]\n",
      " [ 4.]\n",
      " [31.]\n",
      " [16.]\n",
      " [35.]\n",
      " [17.]\n",
      " [34.]\n",
      " [30.]\n",
      " [29.]\n",
      " [24.]\n",
      " [19.]\n",
      " [18.]\n",
      " [28.]\n",
      " [33.]\n",
      " [16.]\n",
      " [ 3.]\n",
      " [ 7.]\n",
      " [26.]\n",
      " [14.]\n",
      " [28.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [26.]\n",
      " [ 8.]\n",
      " [33.]\n",
      " [28.]\n",
      " [ 2.]\n",
      " [23.]\n",
      " [31.]\n",
      " [29.]\n",
      " [18.]\n",
      " [31.]\n",
      " [28.]\n",
      " [20.]\n",
      " [29.]\n",
      " [19.]\n",
      " [ 9.]\n",
      " [31.]\n",
      " [11.]\n",
      " [11.]\n",
      " [19.]\n",
      " [20.]\n",
      " [16.]\n",
      " [ 6.]\n",
      " [ 3.]\n",
      " [28.]\n",
      " [ 3.]\n",
      " [ 5.]\n",
      " [29.]]\n"
     ]
    }
   ],
   "source": [
    "pred, W, b = model(X_train, Y_train_converted_to_numbers, word_to_vec_map)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---------------------------------------------RNN - LSTM------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "Accuracy: 0.9612403100775194\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "pred_train = predict(X_train, Y_train_converted_to_numbers, W, b, word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_predictions(X, pred):\n",
    "    print()\n",
    "    for i in range(X.shape[0]):\n",
    "        print(X[i], \" : \" ,Y_train_unique[int(pred[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n",
      "\n",
      "the speed of the sun sun\n",
      "speed of the moon moon\n",
      "the most bright star star\n",
      "what are the orbits and system solar solar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:31: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    }
   ],
   "source": [
    "X_my_sentences = np.array([\"the speed of the sun\", \"speed of the moon\", \"the most bright star\"\n",
    "                           , \"what are the orbits and system solar\"])\n",
    "Y_my_labels = np.array([[0], [0]])\n",
    "pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n",
    "print_predictions(X_my_sentences, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
